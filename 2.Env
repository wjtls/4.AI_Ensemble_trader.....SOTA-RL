class Env:   
    def __init__(self,coin_or_stock): 
        self.price=0   #현재 주식의 주가(향후 RL학습에 사용할 클래스)
        self.coin_or_stock=coin_or_stock
        
        
        
    def env_reset(self): #초기화함수
        self.price=0
        
        
        
    def stock_select(self): #종목을 정하는 함수
        #현재는 수동설정이며 향후 주식,코인을 통틀어 알고리즘에 의한 종목 선택을 할예정
        if self.coin_or_stock=='coin':
            name_='KRW-BTC'  #비트코인
        else:
            name_='SPY' #kindex sp500지수:'A360200', SPY500= 'SPY'
        return name_
    
        
        
    def data_create(self,  #데이터 호출 함수
                    minute, #분봉(코인)
                    data_count ):  #불러올 데이터수  (코인)
        
        name_=self.stock_select()
        print('종목선정완료',name_)
        exchange=1180 #환율
        
        if self.coin_or_stock=='coin':
            #코인데이터 호출
            date = None #초기에 None이면 최근 200개 호출
            dfs = [ ]

            for i in range(data_count // 200 + 1):
                if i < data_count // 200 :
                    df = py.get_ohlcv(name_, to = date, interval = 200) #date 까지 200개 호출
                    date = df.index[0]
                elif data_count % 200 != 0 :    #최근 200개
                    df = py.get_ohlcv(name_, to = date, interval = 200, count = data_count % 200)
                else :
                    break
                dfs.append(df)
                time.sleep(0.1)
                
            data = pd.concat(dfs).sort_index()      #하나의 데이터 프레임으로 묶음
            close_=pd.Series(data['close'].values)
            vol_=pd.Series(data['volume'].values)
                
        if self.coin_or_stock=='stock': #주가데이터 불러온다
            data=fd.DataReader(name_,start='2017',end='2021')
            len_data=len(data)
            data_=data[len(data)-data_count:]
            close_=pd.Series(data_['Close'])*exchange
            vol_=pd.Series(data_['Volume'])/exchange
                
        
        scaler = MinMaxScaler()     #0-1사이로 정규화
        close_1=scaler.fit_transform(close_.values.reshape(-1,1))  
        vol_1=scaler.fit_transform(vol_.values.reshape(-1,1))

        close_=close_      #스케일링 이전 데이터
        vol_=vol_

        close_1=close_1.reshape(-1)    #스케일링 데이터
        vol_1=vol_1.reshape(-1)
        

        return close_,vol_,close_1,vol_1
    
    
    def distribute_data(self,input_,ratio): #훈련,검증,테스트 셋으로 나눔 , ratio 인풋= 비율의 리스트
                                       #ratio= train과 val test 비율
        step_ratio,_=divmod(len(input_),(ratio[0]+ratio[1]+ratio[2]))
        train_ratio=step_ratio*ratio[0]
        val_ratio=step_ratio*ratio[1]
        test_ratio=step_ratio*ratio[2]
        
        train_data=input_[:train_ratio]
        val_data= input_[train_ratio:train_ratio+val_ratio]
        test_data= input_[train_ratio+val_ratio:]
        
        return train_data, val_data, test_data
    
    
    def input_create(self,
                      minute, #minute:분봉(코인)
                     ratio,   #ratio는 데이터셋 비율을 리스트로 넣음
                     day_count,#호출할 데이터수(코인)
                    ):        
        
        
        close_,vol_,close_scale,vol_scale= self.data_create(minute,day_count)         #close와 vol데이터를 가져온다. (0-1사이로스케일링된 데이터도 가져온다)
        
        #feature 들의 기간을 똑같이 맞춘다
        close_len=len(close_scale)
        
        close=torch.Tensor(np.array(close_))
        close_sc=torch.Tensor(np.array(close_scale))
        
        
        #train,val,test set 으로 나눈다
        train_close,val_close,test_close=self.distribute_data(close_sc,ratio)
        train_close_,val_close_,test_close_=self.distribute_data(close,ratio)
        
        #feature들을 가진 input을 생성한다
        train_data=torch.cat([train_close])
        val_data=torch.cat([val_close])
        test_data=torch.cat([test_close])
        ori_close=[train_close_,val_close_,test_close_]
        
        return train_data,val_data,test_data,ori_close         #스케일링된 data set 과 기존 종가(close) 데이터가 나옴
    
    
    def log_return(self,close):    #종가의 로그리턴을 구하는 함수
        #log_return= log(Pt+1)-log(Pt)
        
        series= pd.DataFrame(close)
        log_re=[0]   #다른지표와 인덱스를 맞추기위해 초기에 0을 넣는다
        
        for t in range(len(close)-1):
            return_=np.log(series.iloc[t+1])-np.log(series.iloc[t])
            log_re.append(round(float(return_),5))
            
        return log_re
    
    
    
    def LSTM_observation(self,input_,window):    #윈도우사이즈만큼 텐서형태로 출력 LSTM인풋 데이터
        window_size=window
        data=[] #데이터
        idx=[]  #데이터의 인덱스 모음
        input_=input_.tolist()
        for k in range(len(input_)-(window_size-1)):
            data.append(input_[k:k+window_size])
            idx.append(range(k,k+window_size))
            
        data=torch.Tensor(data).view(-1,1,window) #batch first 일경우      (배치길이(총길이), 디멘션, 시퀀스길이(윈도우사이즈))
        
        
        return data,idx
    
    
    
    def discrete_step(self,action,unit,step,model): #액션이 discrete할때 1스탭
        
        #액션0= 매도 1=관망  2=매수
        #참고 Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy 논문
        
        action=action.item()
        if action==0: #매도일경우 
            if model.stock>=unit[0]:  #주식의 갯수가 팔려는 갯수이상일때만 매도
                model.stock-=unit[0]
                model.cash+=self.price*unit[0]
                
            elif model.stock>0: 
                #가진주식수 부족한데 팔주식은 있는경우
                model.cash+=model.stock*self.price  #전량매도
                unit[0]=model.stock
                model.stock-=unit[0]
                
            else:
                unit[0]=0
            
        elif action==2: #매수일경우
            if model.cash>=unit[2]*self.price:   #가진현금이 사려고하는 가격보다 많을때
                model.stock+=unit[2]
                model.cash-=self.price*unit[2]
            elif model.cash>self.price:                              #유닛대비 현금부족
                quotient,remainder=divmod(float(model.cash),float(model.price))  #몫과 나머지
                model.stock+=quotient
                model.cash-=quotient*self.price
                unit[2]=quotient   
            else:                                      #1주도 못사는경우
                unit[2]=0
            
        else: #관망일경우
            model.cash=model.cash
            model.stock=model.stock
            
        #Rs=다음스테이트에서 팔았을때 리워드
        #Rh="홀딩했을때
        #Rb="매수했을때
        
        Rs=((model.price_data[step+1]-model.price_data[step])/model.price_data[step+1])*(unit[0])  
        Rh=((model.price_data[step+1]-model.price_data[step])/model.price_data[step+1])*(unit[1])
        Rb=((model.price_data[step+1]-model.price_data[step])/model.price_data[step+1])*(unit[2])    #(0-1 사이 가격[t+1] - 0-1사이가격[t])*매수시 수량
        
        reward=Rh-Rs+Rb-((Rs)*model.cost)
        
        cost=(unit[0]*model.price)*model.cost
        model.cash-=cost    #매도시 수수료를 지불
        model.PV=(model.stock*model.price+model.cash) #PV업데이트
        '''''
        이런 reward 계산 이유:
        
        indicates that I need to maximize the positive change
        of the portfolio value by buying and holding the stocks
        whose price will increase at next time step and minimize
        the negative change of the portfolio value by selling the
        stocks whose price will decrease at next time step
        '''''
        
        model.action_data.append([action])
        model.reward_data.append([reward])  #네트워크 출력후 각각 계산해야하므로 차원추가
        model.step_data.append(step)
        #여러 에이전트 돌려야되는 환경이므로 
        #Env가 아닌 에이전트 클래스에 액션 ,리워드 ,스테이트 데이터 저장
            
        return action,reward,step
    
    
    
    
    def continuous_step(self,action,unit,step,model): #action이 continuous 할때 1스탭
        action=action.item()  
        next_step=step+1
        #Rs=next step 에서 팔았을때 reward
        #Rh=holding 했을때 reward
        #Rb="매수했을때 reward
        
        if model.cash>=unit[2]*self.price:   #가진현금이 사려고하는 가격보다 많을때
                model.stock+=unit[2]
                model.cash-=self.price*unit[2]
        elif model.cash>self.price:                              #유닛대비 현금부족
            quotient,remainder=divmod(float(model.cash),float(model.price))  #몫과 나머지
            model.stock+=quotient
            model.cash-=quotient*self.price
            unit[2]=quotient   
        else:                                      #1주도 못사는경우
            unit[2]=0
            
            
        model.cash-=(unit[1]*self.price) #관망인경우 (unit[1]=0)
        
        if model.stock>=unit[0]:  #주식의 갯수가 팔려는 갯수이상일때만 매도
            model.stock-=unit[0]
            model.cash+=self.price*unit[0]

        elif model.stock>0: 
            #가진주식수 부족한데 팔주식은 있는경우
            model.cash+=model.stock*self.price  #전량매도
            unit[0]=model.stock
            model.stock-=unit[0]

        else:
            unit[0]=0
        
        Rs=((model.price_data[step+1]-model.price_data[step])/model.price_data[step+1])*(unit[0])  
        Rh=((model.price_data[step+1]-model.price_data[step])/model.price_data[step+1])*(unit[1])
        Rb=((model.price_data[step+1]-model.price_data[step])/model.price_data[step+1])*(unit[2])    #(0-1 사이 가격[t+1] - 0-1사이가격[t])*매수시 수량
        
        reward=Rh-Rs+Rb-((Rs)*model.cost)
        reward=reward*10
        
        cost=(unit[0]*model.price)*model.cost #매도시 수수료
        model.cash-=cost                      #매도시 수수료를 지불
        model.PV=(model.stock*model.price+model.cash) #PV업데이트
        #여러 에이전트 돌려야되는 환경이므로 
        #Env가 아닌 에이전트 클래스에 액션 ,리워드 ,스테이트 데이터 저장
        
        
        return action,reward,step,next_step
    
    
